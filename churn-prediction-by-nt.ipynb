{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n#                 Customer Churn Prediction by Naima Tanveer\n","metadata":{}},{"cell_type":"markdown","source":"Develop a model to predict customer churn for a subscription-\nbased service or business. Use historical customer data, including\n\nfeatures like usage behavior and customer demographics, and try\nalgorithms like Logistic Regression, Random Forests, or Gradient\n\nBoosting to predict churn.","metadata":{}},{"cell_type":"markdown","source":"# Importing Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom transformers import AutoModel, BertTokenizerFast\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LogisticRegression\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:51:00.208982Z","iopub.execute_input":"2023-11-08T06:51:00.209745Z","iopub.status.idle":"2023-11-08T06:51:00.215320Z","shell.execute_reply.started":"2023-11-08T06:51:00.209710Z","shell.execute_reply":"2023-11-08T06:51:00.214171Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"# Load the dataset\ndata = pd.read_csv(\"/kaggle/input/bank-customer-churn-prediction/Churn_Modelling.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:51:00.223843Z","iopub.execute_input":"2023-11-08T06:51:00.224167Z","iopub.status.idle":"2023-11-08T06:51:00.279694Z","shell.execute_reply.started":"2023-11-08T06:51:00.224140Z","shell.execute_reply":"2023-11-08T06:51:00.278616Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n0          1    15634602  Hargrave          619    France  Female   42   \n1          2    15647311      Hill          608     Spain  Female   41   \n2          3    15619304      Onio          502    France  Female   42   \n3          4    15701354      Boni          699    France  Female   39   \n4          5    15737888  Mitchell          850     Spain  Female   43   \n\n   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n0       2       0.00              1          1               1   \n1       1   83807.86              1          0               1   \n2       8  159660.80              3          1               0   \n3       1       0.00              2          0               0   \n4       2  125510.82              1          1               1   \n\n   EstimatedSalary  Exited  \n0        101348.88       1  \n1        112542.58       0  \n2        113931.57       1  \n3         93826.63       0  \n4         79084.10       0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RowNumber</th>\n      <th>CustomerId</th>\n      <th>Surname</th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>15634602</td>\n      <td>Hargrave</td>\n      <td>619</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>101348.88</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>15647311</td>\n      <td>Hill</td>\n      <td>608</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>41</td>\n      <td>1</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>112542.58</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>15619304</td>\n      <td>Onio</td>\n      <td>502</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113931.57</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>15701354</td>\n      <td>Boni</td>\n      <td>699</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>93826.63</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>15737888</td>\n      <td>Mitchell</td>\n      <td>850</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>2</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>79084.10</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# import BERT-base pretrained model\nbert = AutoModel.from_pretrained('bert-base-uncased')\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:51:00.281380Z","iopub.execute_input":"2023-11-08T06:51:00.281726Z","iopub.status.idle":"2023-11-08T06:51:01.989837Z","shell.execute_reply.started":"2023-11-08T06:51:00.281696Z","shell.execute_reply":"2023-11-08T06:51:01.988841Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"# Encode 'Geography' and 'Gender' columns using BERT embeddings\ndata['Geography_encoded'] = data['Geography'].apply(lambda x: tokenizer.encode(x, add_special_tokens=False))\ndata['Gender_encoded'] = data['Gender'].apply(lambda x: tokenizer.encode(x, add_special_tokens=False))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:51:01.991262Z","iopub.execute_input":"2023-11-08T06:51:01.991702Z","iopub.status.idle":"2023-11-08T06:51:03.049019Z","shell.execute_reply.started":"2023-11-08T06:51:01.991660Z","shell.execute_reply":"2023-11-08T06:51:03.047934Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"In the above code, the 'Geography' and 'Gender' columns in the 'data' DataFrame are being encoded using BERT embeddings. This is done to represent the textual information in these columns as numerical vectors, which can be used as input features for machine learning models. The 'tokenizer.encode' function is used to tokenize and convert the text in these columns into numeric representations without adding special tokens.","metadata":{}},{"cell_type":"code","source":"data.head()","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:51:03.053477Z","iopub.execute_input":"2023-11-08T06:51:03.054083Z","iopub.status.idle":"2023-11-08T06:51:03.077427Z","shell.execute_reply.started":"2023-11-08T06:51:03.054046Z","shell.execute_reply":"2023-11-08T06:51:03.076299Z"},"trusted":true},"execution_count":24,"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"   RowNumber  CustomerId   Surname  CreditScore Geography  Gender  Age  \\\n0          1    15634602  Hargrave          619    France  Female   42   \n1          2    15647311      Hill          608     Spain  Female   41   \n2          3    15619304      Onio          502    France  Female   42   \n3          4    15701354      Boni          699    France  Female   39   \n4          5    15737888  Mitchell          850     Spain  Female   43   \n\n   Tenure    Balance  NumOfProducts  HasCrCard  IsActiveMember  \\\n0       2       0.00              1          1               1   \n1       1   83807.86              1          0               1   \n2       8  159660.80              3          1               0   \n3       1       0.00              2          0               0   \n4       2  125510.82              1          1               1   \n\n   EstimatedSalary  Exited Geography_encoded Gender_encoded  \n0        101348.88       1            [2605]         [2931]  \n1        112542.58       0            [3577]         [2931]  \n2        113931.57       1            [2605]         [2931]  \n3         93826.63       0            [2605]         [2931]  \n4         79084.10       0            [3577]         [2931]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>RowNumber</th>\n      <th>CustomerId</th>\n      <th>Surname</th>\n      <th>CreditScore</th>\n      <th>Geography</th>\n      <th>Gender</th>\n      <th>Age</th>\n      <th>Tenure</th>\n      <th>Balance</th>\n      <th>NumOfProducts</th>\n      <th>HasCrCard</th>\n      <th>IsActiveMember</th>\n      <th>EstimatedSalary</th>\n      <th>Exited</th>\n      <th>Geography_encoded</th>\n      <th>Gender_encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>15634602</td>\n      <td>Hargrave</td>\n      <td>619</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>2</td>\n      <td>0.00</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>101348.88</td>\n      <td>1</td>\n      <td>[2605]</td>\n      <td>[2931]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>15647311</td>\n      <td>Hill</td>\n      <td>608</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>41</td>\n      <td>1</td>\n      <td>83807.86</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>112542.58</td>\n      <td>0</td>\n      <td>[3577]</td>\n      <td>[2931]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>15619304</td>\n      <td>Onio</td>\n      <td>502</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>42</td>\n      <td>8</td>\n      <td>159660.80</td>\n      <td>3</td>\n      <td>1</td>\n      <td>0</td>\n      <td>113931.57</td>\n      <td>1</td>\n      <td>[2605]</td>\n      <td>[2931]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>15701354</td>\n      <td>Boni</td>\n      <td>699</td>\n      <td>France</td>\n      <td>Female</td>\n      <td>39</td>\n      <td>1</td>\n      <td>0.00</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>93826.63</td>\n      <td>0</td>\n      <td>[2605]</td>\n      <td>[2931]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>15737888</td>\n      <td>Mitchell</td>\n      <td>850</td>\n      <td>Spain</td>\n      <td>Female</td>\n      <td>43</td>\n      <td>2</td>\n      <td>125510.82</td>\n      <td>1</td>\n      <td>1</td>\n      <td>1</td>\n      <td>79084.10</td>\n      <td>0</td>\n      <td>[3577]</td>\n      <td>[2931]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Split the data into features and target\nX = data[['CreditScore', 'Age', 'Tenure', 'Balance', 'NumOfProducts', 'HasCrCard', 'IsActiveMember', 'EstimatedSalary',\n          'Geography_encoded', 'Gender_encoded']]\ny = data['Exited']","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:51:03.078646Z","iopub.execute_input":"2023-11-08T06:51:03.078985Z","iopub.status.idle":"2023-11-08T06:51:03.086726Z","shell.execute_reply.started":"2023-11-08T06:51:03.078956Z","shell.execute_reply":"2023-11-08T06:51:03.085516Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:51:03.088110Z","iopub.execute_input":"2023-11-08T06:51:03.088429Z","iopub.status.idle":"2023-11-08T06:51:03.103175Z","shell.execute_reply.started":"2023-11-08T06:51:03.088401Z","shell.execute_reply":"2023-11-08T06:51:03.101864Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"markdown","source":"The above code splits the data into training and testing sets, with 80% of the data used for training (X_train and y_train) and 20% for testing (X_test and y_test). This separation is crucial for assessing the performance of machine learning models by training them on one portion of the data and evaluating their predictions on the other, helping to avoid overfitting and gauge model generalization. The \"test_size\" parameter specifies the proportion of data allocated for testing, and \"random_state\" ensures reproducibility of the split.","metadata":{}},{"cell_type":"code","source":"# Flatten BERT embeddings lists\nX_train['Geography_encoded'] = X_train['Geography_encoded'].apply(lambda x: x[0])\nX_train['Gender_encoded'] = X_train['Gender_encoded'].apply(lambda x: x[0])\nX_test['Geography_encoded'] = X_test['Geography_encoded'].apply(lambda x: x[0])\nX_test['Gender_encoded'] = X_test['Gender_encoded'].apply(lambda x: x[0])\n\n# Standardize numeric features\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:51:03.104793Z","iopub.execute_input":"2023-11-08T06:51:03.105323Z","iopub.status.idle":"2023-11-08T06:51:03.143851Z","shell.execute_reply.started":"2023-11-08T06:51:03.105284Z","shell.execute_reply":"2023-11-08T06:51:03.142814Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"# Logistic Regression\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(X_train, y_train)\n\n# Random Forest\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(X_train, y_train)\n\n# Gradient Boosting\ngradient_boosting = GradientBoostingClassifier()\ngradient_boosting.fit(X_train, y_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:51:59.843021Z","iopub.execute_input":"2023-11-08T06:51:59.843410Z","iopub.status.idle":"2023-11-08T06:52:02.547695Z","shell.execute_reply.started":"2023-11-08T06:51:59.843381Z","shell.execute_reply":"2023-11-08T06:52:02.546505Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"GradientBoostingClassifier()","text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier()</pre></div></div></div></div></div>"},"metadata":{}}]},{"cell_type":"code","source":"# Logistic Regression Evaluation\nlogistic_regression_predictions = logistic_regression.predict(X_test)\nprint(\"Logistic Regression Classification Report:\")\nprint(classification_report(y_test, logistic_regression_predictions))\n\n# Random Forest Evaluation\nrandom_forest_predictions = random_forest.predict(X_test)\nprint(\"Random Forest Classification Report:\")\nprint(classification_report(y_test, random_forest_predictions))\n\n# Gradient Boosting Evaluation\ngradient_boosting_predictions = gradient_boosting.predict(X_test)\nprint(\"Gradient Boosting Classification Report:\")\nprint(classification_report(y_test, gradient_boosting_predictions))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-08T06:52:30.841075Z","iopub.execute_input":"2023-11-08T06:52:30.841493Z","iopub.status.idle":"2023-11-08T06:52:30.986957Z","shell.execute_reply.started":"2023-11-08T06:52:30.841464Z","shell.execute_reply":"2023-11-08T06:52:30.985852Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stdout","text":"Logistic Regression Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.83      0.97      0.89      1607\n           1       0.60      0.17      0.27       393\n\n    accuracy                           0.81      2000\n   macro avg       0.71      0.57      0.58      2000\nweighted avg       0.78      0.81      0.77      2000\n\nRandom Forest Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.97      0.92      1607\n           1       0.76      0.46      0.57       393\n\n    accuracy                           0.87      2000\n   macro avg       0.82      0.71      0.75      2000\nweighted avg       0.86      0.87      0.85      2000\n\nGradient Boosting Classification Report:\n              precision    recall  f1-score   support\n\n           0       0.88      0.96      0.92      1607\n           1       0.75      0.47      0.58       393\n\n    accuracy                           0.87      2000\n   macro avg       0.82      0.72      0.75      2000\nweighted avg       0.86      0.87      0.85      2000\n\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Conclusion:The Gradient Boosting model performs similarly to the Random Forest model, with high precision and recall for class 0 and class 1. It achieves good accuracy and F1-scores for both non-default and default cases.\n\n In summary, the Random Forest and Gradient Boosting models outperform the Logistic Regression model, with better precision, recall, and F1-scores for both classes. They offer a more balanced trade-off between correctly identifying non-default and default cases. The choice between Random Forest and Gradient Boosting would depend on specific application requirements and the importance of different evaluation metrics.\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}